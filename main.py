from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from datetime import datetime
import pytz
import os
import base64
import tempfile
from dotenv import load_dotenv
from openai import OpenAI
from pathlib import Path
import numpy as np
import cv2

# Load env from Backend folder as user specified
env_path = Path(__file__).parent.parent / "Backend" / ".env"
load_dotenv(dotenv_path=env_path)

api_key = os.getenv("OPENAI_API")
client = OpenAI(api_key=api_key) if api_key else None

app = FastAPI(
    title="AI Kiosk - AI Server",
    description="MediaPipe Poseì™€ FaceMeshìš© ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ë™ ì„œë²„ + OpenAI Integration",
    version="0.2.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def get_timestamp():
    korea_timezone = pytz.timezone("Asia/Seoul")
    return datetime.now(korea_timezone).isoformat()

# --- Common Models ---

class CommonResponse(BaseModel):
    success: bool
    data: Optional[Dict[str, Any]] = None
    error: Optional[Dict[str, Any]] = None
    meta: Optional[Dict[str, Any]] = None
    timestamp: str = ""
    requestId: str = "req_default"

    def __init__(self, **data):
        super().__init__(**data)
        if not self.timestamp:
            self.timestamp = get_timestamp()

# --- DTO Models ---

class SttRequest(BaseModel):
    audioBase64: str
    language: str = "ko"

class TtsRequest(BaseModel):
    text: str
    voice: str = "alloy"
    speed: float = 1.0

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatContext(BaseModel):
    stage: Optional[str] = None
    userType: Optional[str] = None
    sessionId: Optional[str] = None
    kioskState: Optional[str] = None

class LlmChatRequest(BaseModel):
    messages: List[ChatMessage]
    context: Optional[ChatContext] = None

class NluRequest(BaseModel):
    utterance: str

class LlmSuggestRequest(BaseModel):
    hesitationLevel: float
    state: str

class LlmSummarizeRequest(BaseModel):
    history: List[ChatMessage]
    sessionId: str

class VisionFrameRequest(BaseModel):
    frameBase64: str
    sessionId: Optional[str] = None
    maxHands: Optional[int] = 2

class SignLanguageRequest(BaseModel):
    handLandmarks: List[Any]

class Landmark(BaseModel):
    x: float
    y: float
    z: float = 0.0
    visibility: Optional[float] = None

class LandmarkGroup(BaseModel):
    landmarks: List[Landmark]

class HesitationRequest(BaseModel):
    face: Optional[LandmarkGroup] = None
    pose: Optional[LandmarkGroup] = None

# --- YHG-pose Models ---

class HesitationResponse(BaseModel):
    """ë§ì„¤ì„ ê°ì§€ ì‘ë‹µ ëª¨ë¸"""
    hesitation_level: int
    confidence: float
    label: str
    probabilities: Optional[List[float]] = None
    error: Optional[str] = None


class SignLanguageResponse(BaseModel):
    """ìˆ˜í™” ì¸ì‹ ì‘ë‹µ ëª¨ë¸"""
    text: str
    error: Optional[str] = None


class Base64ImageRequest(BaseModel):
    """Base64 ì´ë¯¸ì§€ ìš”ì²­ ëª¨ë¸"""
    image: str  # Base64 encoded image
    binary: bool = False  # ì´ì§„ ë¶„ë¥˜ ëª¨ë“œ

# --- Endpoints ---

@app.get("/", response_class=HTMLResponse)
async def root():
    return """
    <html>
    <body>
        <div>
            <h1>ğŸ¤– AI Kiosk - AI Server</h1>
            <p>FastAPI Server is running with OpenAI Integration.</p>
            <p class="tech">Python + FastAPI + MediaPipe + OpenAI</p>
            <p style="margin-top: 2rem; font-size: 0.9rem; opacity: 0.7">FusionCrew Â© 2025~2026</p>
        </div>
    </body>
    </html>
    """

@app.get("/health")
async def health():
    return {"status": "ok"}

@app.get("/api/ping")
async def ping():
    return {"message": "pong", "server": "ai-server", "openai_connected": bool(client)}

@app.get("/api/v1/meta/health")
async def meta_health():
    return {"status": "UP"}

@app.get("/api/v1/meta/models")
async def meta_models():
    return {
        "models": [
            {"name": "whisper-1", "loaded": True, "provider": "OpenAI"},
            {"name": "tts-1", "loaded": True, "provider": "OpenAI"},
            {"name": "gpt-3.5-turbo", "loaded": True, "provider": "OpenAI"},
            {"name": "mediapipe", "loaded": True, "provider": "Local"},
            {"name": "hesitation-model", "loaded": True, "provider": "Local"}, # Added
        ]
    }

# 1. STT (OpenAI Whisper)
@app.post("/api/v1/stt")
async def stt(request: SttRequest):
    if not client:
        return CommonResponse(success=False, error={"code": "NO_API_KEY", "message": "OpenAI API Key not found"})

    try:
        # Decode Base64 to temp file
        audio_data = base64.b64decode(request.audioBase64)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as temp_audio:
            temp_audio.write(audio_data)
            temp_audio_path = temp_audio.name

        # Call OpenAI Whisper
        with open(temp_audio_path, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language=request.language
            )
        
        # Cleanup
        os.remove(temp_audio_path)

        return CommonResponse(
            success=True,
            data={
                "text": transcript.text,
                "confidence": 0.99 
            },
            meta={"model": "whisper-1", "provider": "openai"},
            requestId=f"req_stt_{int(datetime.now().timestamp())}"
        )
    except Exception as e:
        return CommonResponse(success=False, error={"code": "STT_FAILED", "message": str(e)})

# 2. TTS (OpenAI TTS)
@app.post("/api/v1/tts")
async def tts(request: TtsRequest):
    if not client:
        return CommonResponse(success=False, error={"code": "NO_API_KEY", "message": "OpenAI API Key not found"})

    try:
        response = client.audio.speech.create(
            model="tts-1",
            voice=request.voice if request.voice in ["alloy", "echo", "fable", "onyx", "nova", "shimmer"] else "alloy",
            input=request.text,
            speed=request.speed
        )
        
        # Binary to Base64
        audio_content = response.content
        audio_base64 = base64.b64encode(audio_content).decode('utf-8')

        return CommonResponse(
            success=True,
            data={"audioBase64": audio_base64},
            meta={"model": "tts-1", "provider": "openai"},
            requestId=f"req_tts_{int(datetime.now().timestamp())}"
        )
    except Exception as e:
        return CommonResponse(success=False, error={"code": "TTS_FAILED", "message": str(e)})

# 3. Chat (Mock for now, can perform actual LLM call if needed)
@app.post("/api/v1/llm/chat")
async def llm_chat(request: LlmChatRequest):
    return CommonResponse(
        success=True,
        data={
            "reply": f"AI ì‘ë‹µì…ë‹ˆë‹¤: {request.messages[-1].content}ì— ëŒ€í•´ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.",
            "intent": "GENERAL"
        },
        requestId="req_chat_01"
    )

# 4. NLU
@app.post("/api/v1/nlu/parse")
async def nlu_parse(request: NluRequest):
    return CommonResponse(
        success=True,
        data={
            "intent": "ORDER",
            "slots": {"menu": [{"name": "Mock Item", "quantity": 1}]}
        },
        requestId="req_nlu_01"
    )

# 5. Suggest
@app.post("/api/v1/llm/suggest")
async def llm_suggest(request: LlmSuggestRequest):
    return CommonResponse(
        success=True,
        data={"suggestion": "ë©”ë‰´ ì„ íƒì´ ì–´ë ¤ìš°ì‹œë©´ ì¶”ì²œì„ ë„ì™€ë“œë¦´ê²Œìš”."},
        requestId="req_suggest_01"
    )

# 6. Summarize
@app.post("/api/v1/llm/summarize")
async def llm_summarize(request: LlmSummarizeRequest):
    return CommonResponse(
        success=True,
        data={
            "summary": "Mock Summary",
            "compressedContext": {"lastTopic": "Mock"}
        },
        requestId="req_summarize_01"
    )

# 7 ~ 11. Vision (Mocks kept as is)
@app.post("/api/v1/vision/facemesh")
async def vision_facemesh(request: VisionFrameRequest):
    return CommonResponse(success=True, data={"landmarks": [], "count": 0}, requestId="req_facemesh")

@app.post("/api/v1/vision/pose")
async def vision_pose(request: VisionFrameRequest):
    return CommonResponse(success=True, data={"landmarks": []}, requestId="req_pose")

@app.post("/api/v1/vision/hands")
async def vision_hands(request: VisionFrameRequest):
    return CommonResponse(success=True, data={"hands": []}, requestId="req_hands")

@app.post("/api/v1/vision/sign-language/interpret")
async def sign_language(request: SignLanguageRequest):
    return CommonResponse(success=True, data={"commands": []}, requestId="req_sign")
    
@app.post("/api/v1/vision/hesitation")
async def hesitation(request: HesitationRequest):
    return CommonResponse(success=True, data={"score": 0.0, "level": "LOW", "signals": []}, requestId="req_hesitation")


# ============================================
# Hesitation Detection API (From YHG-pose)
# ============================================

@app.post("/api/hesitation/detect", response_model=HesitationResponse)
async def detect_hesitation_from_image(image: UploadFile = File(...)):
    """
    ì´ë¯¸ì§€ì—ì„œ ë§ì„¤ì„ ì •ë„ ê°ì§€
    
    - **image**: ì´ë¯¸ì§€ íŒŒì¼ (JPEG, PNG ë“±)
    - Returns: ë§ì„¤ì„ ë ˆë²¨ (0-3), ì‹ ë¢°ë„, ë¼ë²¨
    """
    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        contents = await image.read()
        nparr = np.frombuffer(contents, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if img is None:
            raise HTTPException(status_code=400, detail="Failed to decode image")
        
        # ë§ì„¤ì„ ê°ì§€
        from hesitationLearning.inference import detect_hesitation
        result = detect_hesitation(img, binary=False)
        
        return HesitationResponse(**result)
        
    except ImportError:
        raise HTTPException(
            status_code=503,
            detail="Hesitation detection model not available. Please train the model first."
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/hesitation/detect-base64", response_model=HesitationResponse)
async def detect_hesitation_from_base64(request: Base64ImageRequest):
    """
    Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ì—ì„œ ë§ì„¤ì„ ê°ì§€
    
    - **image**: Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¬¸ìì—´
    - **binary**: Trueë©´ ì´ì§„ ë¶„ë¥˜ (ë§ì„¤ì„/ë¹„ë§ì„¤ì„)
    - Returns: ë§ì„¤ì„ ë ˆë²¨, ì‹ ë¢°ë„, ë¼ë²¨
    """
    try:
        from hesitationLearning.inference import get_detector
        detector = get_detector(binary=request.binary)
        result = detector.detect_from_base64(request.image)
        
        return HesitationResponse(**result)
        
    except ImportError:
        raise HTTPException(
            status_code=503,
            detail="Hesitation detection model not available. Please train the model first."
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/hesitation/status")
async def hesitation_model_status():
    """ë§ì„¤ì„ ê°ì§€ ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    from pathlib import Path
    from hesitationLearning.config import MODEL_PATH, SCALER_PATH
    
    model_exists = MODEL_PATH.exists()
    scaler_exists = SCALER_PATH.exists()
    
    return {
        "model_available": model_exists and scaler_exists,
        "model_path": str(MODEL_PATH),
        "message": "Model ready" if (model_exists and scaler_exists) else "Model not trained yet"
    }


# ============================================
# Sign Language Translation API (From YHG-pose)
# ============================================

@app.post("/api/sign-language/translate", response_model=SignLanguageResponse)
async def translate_sign_language(video: UploadFile = File(...)):
    """
    ìˆ˜í™” ë¹„ë””ì˜¤ ë²ˆì—­ API
    
    - **video**: ë¹„ë””ì˜¤ íŒŒì¼ (MP4, AVI ë“±)
    - Returns: ë²ˆì—­ëœ í…ìŠ¤íŠ¸
    """
    import tempfile
    import os
    from signLanguage.inference import HandTranslator
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        content = await video.read()
        tmp.write(content)
        tmp_path = tmp.name
        
    try:
        translator = HandTranslator()
        # ë¹„ë””ì˜¤ ì²˜ë¦¬
        result_text = translator.process_video(tmp_path)
        
        if result_text is None:
             raise HTTPException(status_code=400, detail="Could not process video")
             
        return SignLanguageResponse(text=result_text)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(tmp_path):
            os.remove(tmp_path)
